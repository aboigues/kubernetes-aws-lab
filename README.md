# Kubernetes AWS Lab

Infrastructure as Code pour dÃ©ployer des clusters Kubernetes isolÃ©s sur AWS pour des sessions de formation ou de test.

## Vue d'ensemble

Ce projet permet de dÃ©ployer automatiquement plusieurs clusters Kubernetes multi-nÅ“uds isolÃ©s sur AWS, chaque cluster Ã©tant dÃ©diÃ© Ã  un participant. Les clusters partagent un VPC commun mais sont complÃ¨tement isolÃ©s les uns des autres via des security groups dÃ©diÃ©s.

### Architecture

- **VPC partagÃ©** : Un seul VPC pour tous les clusters
- **Clusters isolÃ©s** : Un cluster Kubernetes par participant avec :
  - 1 master node (t3.medium par dÃ©faut)
  - 2 worker nodes (t3.small par dÃ©faut)
- **AccÃ¨s SSH** : Chaque participant utilise sa propre clÃ© SSH ed25519
- **RÃ©seau** : Chaque cluster a son propre pod network CIDR pour Ã©viter les conflits

### Composants

- **Kubernetes 1.28** avec kubeadm
- **Containerd** comme container runtime
- **Calico** comme CNI (Container Network Interface)
- **Ubuntu 22.04 LTS** sur toutes les instances

## PrÃ©requis

- [Terraform](https://www.terraform.io/downloads) >= 1.0
- [AWS CLI](https://aws.amazon.com/cli/) configurÃ© avec les credentials appropriÃ©s
- Droits AWS pour crÃ©er VPC, EC2, Security Groups, etc.

## Installation rapide

### 1. DÃ©poser votre clÃ© SSH publique

Chaque participant doit dÃ©poser sa clÃ© SSH publique dans le rÃ©pertoire de session appropriÃ© :

```bash
# GÃ©nÃ©rer une clÃ© ed25519 si nÃ©cessaire
ssh-keygen -t ed25519

# CrÃ©er votre fichier de clÃ© dans le rÃ©pertoire de session
cat ~/.ssh/id_ed25519.pub > participants/session-XXXXX/prenom.nom.pub

# Commiter
git add participants/session-XXXXX/prenom.nom.pub
git commit -m "Add SSH key for prenom.nom"
git push
```

**Note :** Le nom du fichier sera automatiquement transformÃ© en format discret (prÃ©nom + 2 lettres du nom) pour prÃ©server la confidentialitÃ©.

Voir [participants/README.md](participants/README.md) pour plus de dÃ©tails.

### 2. Valider les clÃ©s SSH

```bash
# Valider les clÃ©s d'une session spÃ©cifique
./scripts/validate-ssh-keys.sh participants/session-XXXXX

# Ou valider toutes les clÃ©s dans participants/ (racine uniquement)
./scripts/validate-ssh-keys.sh
```

### 3. DÃ©ployer l'infrastructure

```bash
cd terraform

# Initialiser Terraform
terraform init

# Voir ce qui va Ãªtre crÃ©Ã©
terraform plan

# DÃ©ployer
terraform apply
```

### 4. AccÃ©der Ã  votre cluster

AprÃ¨s le dÃ©ploiement, Terraform affichera les informations de connexion :

```bash
# RÃ©cupÃ©rer l'IP de votre master node
terraform output

# Se connecter en SSH
ssh ubuntu@<master-ip>

# VÃ©rifier le cluster
kubectl get nodes
kubectl get pods --all-namespaces
```

## Structure du projet

```
kubernetes-aws-lab/
â”œâ”€â”€ README.md                          # Ce fichier
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ SESSION-MANAGEMENT.md          # Guide gestion des sessions
â”‚   â””â”€â”€ PARTICIPANT-ACCESS-SOLUTIONS.md # Solutions de distribution des accÃ¨s
â”œâ”€â”€ participants/                      # ClÃ©s SSH publiques des participants
â”‚   â”œâ”€â”€ README.md                      # Instructions pour les participants
â”‚   â”œâ”€â”€ example.user.pub.example       # Exemple de clÃ© (Ã  renommer en .pub)
â”‚   â””â”€â”€ session-XXXXX/                 # RÃ©pertoire par session
â”‚       â”œâ”€â”€ README.md                  # Infos de la session
â”‚       â”œâ”€â”€ example.user.pub.example   # Exemple de clÃ© (Ã  renommer en .pub)
â”‚       â””â”€â”€ prenom.nom.pub             # ClÃ©s des participants
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate-ssh-keys.sh           # Script de validation des clÃ©s
â”‚   â””â”€â”€ generate-access-info.sh        # GÃ©nÃ©ration des infos d'accÃ¨s
â””â”€â”€ terraform/                         # Infrastructure Terraform
    â”œâ”€â”€ main.tf                        # Configuration principale
    â”œâ”€â”€ variables.tf                   # Variables Terraform
    â”œâ”€â”€ outputs.tf                     # Outputs Terraform
    â”œâ”€â”€ terraform.tfvars.example       # Exemple de configuration
    â””â”€â”€ modules/
        â”œâ”€â”€ vpc/                       # Module VPC partagÃ©
        â”‚   â”œâ”€â”€ main.tf
        â”‚   â”œâ”€â”€ variables.tf
        â”‚   â””â”€â”€ outputs.tf
        â””â”€â”€ k8s-cluster/               # Module cluster Kubernetes
            â”œâ”€â”€ main.tf
            â”œâ”€â”€ variables.tf
            â”œâ”€â”€ outputs.tf
            â”œâ”€â”€ user-data-master.sh    # Script d'init master node
            â””â”€â”€ user-data-worker.sh    # Script d'init worker node
```

## Configuration

### Variables Terraform principales

Vous pouvez personnaliser le dÃ©ploiement en crÃ©ant un fichier `terraform.tfvars` (voir `terraform.tfvars.example`) :

```hcl
# Session identifier (pour le suivi des coÃ»ts)
session_name = "session-nov-2025"

# RÃ©gion AWS
aws_region = "eu-west-1"

# Configuration rÃ©seau
vpc_cidr = "10.0.0.0/16"
availability_zones = ["eu-west-1a", "eu-west-1b"]

# SÃ©curitÃ© - Restreindre l'accÃ¨s SSH et API (recommandÃ©)
allowed_ssh_cidrs = ["0.0.0.0/0"]  # IPs autorisÃ©es pour SSH
allowed_api_cidrs = ["0.0.0.0/0"]  # IPs autorisÃ©es pour Kubernetes API

# Type d'instances
instance_type_master = "t3.medium"  # 2 vCPU, 4 GB RAM
instance_type_worker = "t3.small"   # 2 vCPU, 2 GB RAM

# Nombre de workers par cluster
worker_count = 2

# Version Kubernetes
kubernetes_version = "1.28"

# Nom du projet
project_name = "k8s-lab"
```

### CoÃ»ts estimÃ©s

Pour un dÃ©ploiement avec 5 participants en eu-west-1 :

- 5 master nodes (t3.medium) : ~$0.0416/heure Ã— 5 = ~$0.21/heure
- 10 worker nodes (t3.small) : ~$0.0208/heure Ã— 10 = ~$0.21/heure
- NAT Gateway : ~$0.045/heure Ã— 2 = ~$0.09/heure
- Total : **~$0.51/heure** (~$12/jour)

âš ï¸ **Important** : Pensez Ã  dÃ©truire l'infrastructure quand vous ne l'utilisez plus !

```bash
cd terraform
terraform destroy
```

## Utilisation

### Commandes utiles sur le master node

```bash
# Voir les nÅ“uds du cluster
kubectl get nodes

# Voir tous les pods
kubectl get pods --all-namespaces

# DÃ©ployer une application de test
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80 --type=NodePort

# Voir les services
kubectl get services

# Informations du cluster
/home/ubuntu/cluster-info.sh
```

### Ajouter un participant

1. Le participant ajoute sa clÃ© dans `participants/session-XXXXX/`
2. Valider avec `./scripts/validate-ssh-keys.sh participants/session-XXXXX`
3. Appliquer les changements : `cd terraform && terraform apply`

### Retirer un participant

1. Supprimer le fichier de clÃ© du participant dans `participants/session-XXXXX/`
2. Appliquer les changements : `cd terraform && terraform apply`

Le cluster du participant sera automatiquement dÃ©truit.

## RÃ©solution de problÃ¨mes

### Les worker nodes ne rejoignent pas le cluster

SSH sur le worker node et vÃ©rifier les logs :

```bash
ssh ubuntu@<worker-ip>
sudo tail -f /var/log/user-data.log
```

### Le cluster n'est pas accessible

VÃ©rifier les security groups dans la console AWS.

### ProblÃ¨mes de rÃ©seau

VÃ©rifier que Calico est bien dÃ©ployÃ© :

```bash
kubectl get pods -n kube-system | grep calico
```

## Gestion des Sessions de Formation

Ce projet supporte maintenant la gestion de sessions de formation avec :

- **ğŸ“ Organisation par session** : CrÃ©ez des sous-rÃ©pertoires dans `participants/` pour chaque session
- **ğŸ’° Suivi des coÃ»ts AWS** : Tags automatiques par session pour AWS Cost Explorer
- **ğŸ“¨ Distribution automatique** : Script pour gÃ©nÃ©rer et distribuer les accÃ¨s aux participants
- **ğŸ”’ SÃ©curitÃ© configurable** : VPC et Security Groups paramÃ©trables

### DÃ©marrage rapide pour une session

```bash
# 1. CrÃ©er une session
mkdir -p participants/session-nov-2025

# 2. Les participants ajoutent leurs clÃ©s
# participants/session-nov-2025/prenom.nom.pub

# 3. Configurer Terraform
cat > terraform/terraform.tfvars << EOF
session_name = "session-nov-2025"
allowed_ssh_cidrs = ["0.0.0.0/0"]  # ou IP spÃ©cifique
allowed_api_cidrs = ["0.0.0.0/0"]
EOF

# 4. DÃ©ployer
cd terraform && terraform apply

# 5. Distribuer les accÃ¨s
cd .. && ./scripts/generate-access-info.sh
```

### Documentation dÃ©taillÃ©e

- **[Guide de gestion des sessions](docs/SESSION-MANAGEMENT.md)** - Configuration et organisation des sessions
- **[Solutions de communication](docs/PARTICIPANT-ACCESS-SOLUTIONS.md)** - Comment distribuer les accÃ¨s aux participants

### Suivi des coÃ»ts AWS

Toutes les ressources sont automatiquement taguÃ©es avec `Session = <nom-session>` :

```bash
# Dans AWS Cost Explorer
Filtrer par Tag: Session = session-nov-2025

# Ou via Terraform
cd terraform && terraform output session_info
```

## AmÃ©liorations possibles

- [ ] Utiliser un bastion host au lieu d'exposer les masters publiquement
- [ ] Ajouter un monitoring (Prometheus/Grafana)
- [ ] Configurer des backups automatiques avec Velero
- [ ] Utiliser un load balancer pour l'API server
- [ ] Ajouter des exemples d'applications Ã  dÃ©ployer
- [ ] ImplÃ©menter des quotas par namespace
- [ ] Ajouter des dashboards Kubernetes (Dashboard officiel)

## SÃ©curitÃ©

âš ï¸ **Attention** : Cette configuration est prÃ©vue pour des environnements de formation/test. Pour la production :

- Ne pas exposer les master nodes publiquement
- Utiliser un bastion host ou VPN
- Activer l'audit logging
- ImplÃ©menter des Network Policies strictes
- Utiliser des secrets chiffrÃ©s (KMS)
- Activer le chiffrement des disques EBS
- Configurer des rÃ´les IAM appropriÃ©s

## Licence

MIT

## Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.
